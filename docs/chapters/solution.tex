This chapter first explains the challenges any proposed solution should attempt
to solve (\hyperref[sec:solution-challenges]{section 4.1}). Afterwards, we explain the
solution we implemented (\hyperref[sec:implemented-solution]{section 4.2}) and discuss
alternative solutions that we did not choose
(\hyperref[sec:alternative-approaches]{section 4.3}). Finally, we talk about the object
tracking part of our solution (\hyperref[sec:visual-object-tracking]{section 4.4}) and we
give an overview of the structure of our application
(\hyperref[sec:application-overview]{section 4.5}).
A diagram showing the use cases for each of the four different
tracking models we created is shown in Figure \ref{fig:model-use-case-diagram}.

\section{Challenges that must be overcome}\label{sec:solution-challenges}
In this section, we describe two groups of constraints that any solution to the
problem described in \hyperref[ch:problem-analysis]{chapter 2} has to try to solve. The
first group contains everything related to technology and the second group
contains everything related to the user.

\subsection{Technical limitations}
As outlined in the requirements, the system should act as a middleware device
that can be put on the same network as the microphone and the camera. This adds
a limitation on the amount of available computational resources. We could rely on cloud
computing power, but that would lead to a big delay in camera movement and
additional risks to security and maintainability. Additionally, our project
relies on three special hardware devices -- a camera, and a microphone and a
single-board computer (SBC). The final product should account for all the inherent
limitations of these devices. 

It is important to understand that microphone allows 
us to only retrieve information provided by the Sennheiser which is limited to 
direction toward the speaker and peak loudness level, since we do not have access to each individual microphone
within the array. The biggest limitation is the fact that the
microphone does not provide the exact origin of the detected sound, as it is
quite a difficult technical challenge to calculate
this\cite{distance-urbana-champaign}. 

We formulated three limitations that significantly impact the project:
\begin{itemize}
    \item The storage and processing power of the SBC.
    \item The network range and speed.
    \item The lack of a distance towards the speaker provided by the microphone.
\end{itemize}

\subsection{User limitations}
In addition to technical criteria, the main criterion of the solution we were
going to implement is based on a certain non-functional requirement which Mr.
Kahawati has emphasized multiple times during our meetings: the system should be
easy to install and calibration can be done by teachers or presenters
themselves. It should not require any technical knowledge or external devices.

As this criterion can be interpreted ambiguously (the difficulty of calibration
can vary per person) we needed to reiterate our solutions multiple times and
verify them with Avonic. Because of the difficulty of this criterion, we spent
the entire first week on figuring out a solution that did not disrupt the
simplicity of calibration and installation. The solution we came up with is
presented in the next section.

\section{Implemented solution}\label{sec:implemented-solution}
In this section we give the motivation and reasoning behind our implemented solution.
\subsection{Assumptions}
As described earlier, there are certain challenges that we needed to overcome
in order to devise a working solution. Having brainstormed different
implementation strategies, we identified that the most difficult limitation is
that the microphone array cannot retrieve distance information. This, combined
with the fact that the microphone array is in a separate position from the
camera, meant that generally finding out where the speaker is located becomes
impossible without hacking the microphone to get access to each individual
microphone within the array. That can be easily shown when one imagines a
speaker, who starts off standing directly beneath the microphone array, sitting
down. Our program will then not be able to discern whether to tilt the camera
up or down based solely on the information provided by the ceiling microphone.
This means that we need to make assumptions about the environment of the room:


\begin{enumerate}
    \item The distance between the speaker and the plane of the ceiling, to which the microphone is mounted, is constant.
    \item If a camera, a microphone or the ``speaker plane'' (the horizontal plane in which the speaker moves) were moved, the system must be recalibrated.
    \item The system is well-calibrated and no components of it are moved during the tracking.
\end{enumerate}
Through the process of the research and reiteration of our proposals, we found
this set of assumptions to be the smallest set possible given our limited time
and the limitations mentioned in \hyperref[sec:solution-challenges]{section
4.1}. While assumptions 2 and 3 are in place to guarantee the intended way of
using the system, assumption 1 is formulated to determine the distance from the
microphone to the speaker.

From all of our attempts to come up with the solution, we have identified that
the distance of the vector from the microphone to the speaker is crucial.
Since the microphone's API\cite{microphone-manual} does not have a way to
retrieve this distance, the assumptions are needed to describe the distance
function in the environment.
According to our assumptions, the speaker plane represents the set of all
locations at which the speaker could be. Small deviations from the theoretical
horizontal speaker plane and the actual places at which people speak can be
neglected due to their small magnitude \textbf{(MAYBE PUT REFERENCE HERE)}.
The height difference between speakers can also be neglected due to the
expected distance between the camera and the presenters of at least \textbf{(SOME ASSUMED MINIMUM???)}
meters. Such differences in height have even less of an impact when
the speakers are sitting \textbf{(MAYBE PUT REFERENCE HERE)}.
With this in mind, we decided to
calculate the intersection of the ray from the microphone with the speaker plane
to find the location of the speaker.
In the future, it is possible to extend the model to support different shapes and
possibly a full 3D scan of the room to represent the possible locations of the speaker.
We view these options to be infeasible for us in 10 weeks.

\subsection{Calibration}\label{subsec:calibration}
During the calibration stage, we calculate the distance from the camera to the microphone and therefore obtain the camera's relative coordinates to the microphone.
(We assume the longest one of the horizontal coordinates to be the $z$ one.)
To achieve this, the user first sets microphone's height above the speaker plane.
Then, a person is sent into the room. They direct the camera at themselves and speak.
The direction obtained from the camera and microphone combined with the microphone's height above the so-called speaker plane is enough to calculate the 3D position of the camera relative to the microphone.

A simplified, 2D image is shown in figure \ref{fig:cal2d}. Since the camera
points towards the z-axis by default, the horizontal axis in the image is the z-axis.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{calibration_2d}
    \caption{Demonstration of the calculation of calibration in 2D, where $M$ is the microphone, $C$ is the camera and $A$ is the speaker}
    \label{fig:cal2d}
\end{figure}
We define $M$ as the microphone, $C$ as the camera, and $A$ as the person calibrating the system.
After directing the camera at the speaker, we find out angles $\alpha$, $\beta$ and distance $a$ from the right-angled triangle between the speaker plane, the one perpendicular to it from the microphone, and the speaker.
Angle $\delta$ is easily calculated from the fact that the primary axes of the camera and microphone are orthogonal, and because we know $\gamma$ from having directed the camera at the microphone in the second stage of the calibration.
Now that we know two angles and one side length of the triangle $CAM$, we can calculate the distance between the camera and microphone -- $d$.

After calibration, we can proceed backwards to calculate the angle to point the camera at from $d$.
Figure \ref{fig:cal3d} contains a visualization of the same approach, but with vectors instead of angles.
We can calculate distance $|MA|$ by intersecting the microphone's direction vector with the speaker plane.
Afterwards, we can use analogous calculations to get the length $|CM|$, since we have all the directions we need.
This means that we can find the angles between them and calculate the triangle.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{calibration_3d}
    \caption{Calculation of calibration in 3D}
    \label{fig:cal3d}
\end{figure}

\section{Alternative approaches}\label{sec:alternative-approaches}
Besides the solution using calibration described above, we considered three
other approaches. We used these as sources of inpsiration to come up with our
final solution. These three alternatives are explained in this section together
with a reasoning of why we did not choose them as our final solution.

\subsection{Single coordinate system}
One of the most obvious solutions that we came up with, was to provide the
system with coordinates of both microphone and the camera, in coordination with
the microphone providing distance to the speaker. This solution
served as inspiration for the final solution, but it has two important
problems:
\begin{itemize}
    \item The distance information from the microphone is not obtainable.
    \item Manually inputting measures that would describe a coordinate system
        with coordinates of the camera and the microphone was considered too complex
        by Avonic.
\end{itemize}
\subsection{Interpolation model}
Another solutions we came up with, was inspired by a concept of interpolation.
The proposed solution followed the following algorithm for calibration and tracking:
\begin{enumerate}
    \item Record three positions by retrieving the microphone's beams and the camera's position for each of the recorded placements.
    \item Interpolate a new microphone direction using the three recorded beams and use this to calculate the
            corresponding camera direction.
\end{enumerate}
Although this solution might sound fascinating and simple, it does not work in reality.
This algorithm would only be possible if the camera was located at the same point as the microphone, or in completely opposite position from the microphone, in relation to the calibration points.
This can be proven with a geometric proof, or using geometric modeling that we did with GeoGebra\cite{geogebra}.
Unfortunately, the camera and microphone are not always in the same or opposite
positions, as the system should work with any arbitrary position of the camera.
For this reason, we left this solution behind.

\subsection{Room scan}
A room scan was one of the more ambitious solutions we considered at the
beginning of the project. It was somewhat inspired by the solution with preset
locations. The idea is that we input all the possible planes on which the
speaker could be located. For example, if there is an elevated scene somewhere
at the front of the room we input its size and elevation from the floor. Also,
there is the possibility to add locations for chairs, stairs, etc. This way we
have some kind of a room scan with all the objects’ arrangements inside. This
scan gives us a more precise idea of where the speaker could be actually
located. However, this solution has two major drawbacks. First of all, the
direction from the microphone could still result in multiple possible locations
of the speaker when considering its intersection with all the given planes. The
other disadvantage of this approach is that it requires too much work to set up
the system. As we have stated earlier, this does not align with our
requirements.


\section{Visual object tracking}\label{sec:visual-object-tracking}
This section discusses the addition of visual object tracking to our audio-based speaker tracking
system. The combination of both visual and audio information can make the movement of the camera
smoother and more accurate. Audio discriminates between different people to determine who is
speaking; video helps to exactly center the camera on this person's face.
\subsection{Object tracking methods}
For the visual object tracking part of our system, we considered two methods: YOLO and RT-DETR. Both
are neural networks trained to localize and classify objects in a video. The reason we looked at
neural networks instead of traditional image processing techniques is because neural networks
achieve greater accuracy when large training sets are
available\cite{image-processing-vs-deep-learning}. Such training sets (e.g. VOC 2007\cite{pascal-voc-2007} and MS COCO\cite{microsoft-coco}) are
available for object detection and many models have been trained on these sets. We elaborate on two of such models below.


\paragraph{YOLO}\mbox{} \\
The YOLO (You Only Look Once) network is one of the most common ways to track objects in real-time. YOLO sees the
problem of finding bounding boxes and classifying objects in an image as a regression problem\cite{you-only-look-once}.
The input image is divided into a grid. The network predicts three things for each box in the grid:
first, a constant number of bounding boxes that have their center in the grid; second, a number
representing the class the object in
the box belongs to; third, a confidence score for that class prediction. As such, the output of the network is a vector of
constant size. Since the network often outputs many overlapping boxes, non-maximum suppression is
used. This is a technique that takes a group of boxes with the center in the same square and only
keeps the best performing one. The performance of objects during non-maximum suppression is based on
the intersection-over-union score. This is the score calculated by intersecting one boxes with the
other boxes and dividing this by the union of those same boxes; the higher the score, the more
accurate the prediction.

Considering the YOLO network was made for real-time performance, it fits well with our system. During the
tests described in the YOLO paper, the network can process 45 frames per second\cite{you-only-look-once}.
Moreover, the classification part of the network helps to filter out objects that are not people. This helps to
find out which bounding box corresponds to the speaker.


\paragraph{RT-DETR}\mbox{} \\
The Real-Time Detection Transformer is the first real-time transformer-based detector. In contrast to the convolutional
neural network (CNN) detectors, transformer-based detectors do not need a post-processing step with non-maximum
suppression. This way, RT-DETR manages to achieve a higher speed than YOLO while not losing any accuracy\cite{rt-detr}.

Just like YOLO, RT-DETR can be used to track objects in real-time to assist our overall speaker tracking. However,
despite its claim to improved performance, we noticed that in practice the YOLO algorithm was multiple times faster
than the RT-DETR. This is why we ended up using the latest available version of YOLO (YOLOv8) as our object detector.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{model-diagram}
    \caption{Use case diagram of the tracking models}
    \label{fig:model-use-case-diagram}
\end{figure}

\subsection{Integration of object tracking into our speaker tracker}
The problem of combining two systems of tracking a speaker, both object tracking and speaker tracking, is discussed in
this section. A decision has to be made by the system whether to listen to the speaker tracker or the object tracker.
There are three strategies we implemented in our system to deal with this problem. We review each of them and explain in what kind of situation the strategies work best. Avonic has asked us to give audio-based speaker tracking priority over object detection using video frames. Because of this, all strategies we tried to use audio to determine the location of the speaker for the most important (i.e. biggest) movements.
Since the last two strategies both improve speaker tracking in different ways, the web UI gives users the option to choose between these methods.

\paragraph{Thresholding of camera movements}\mbox{} \\
The first solution we came up with was to threshold incoming commands. This means that we ignore the speaker tracking if
it wants the camera to move a number of degrees lower than the threshold and we ignore the object tracking if it tells
the camera to move a number of degrees higher than the threshold. The envisioned result of this approach would be that
the speaker tracking moves the camera towards the correct speaker and the object tracker then follows the small
movements this speaker makes. This uses both the microphone's ability to detect which person is speaking and the object
detector's ability to precisely localize a person on a frame.

When we tested this strategy, we found out that the simultaneous signals that are still being sent to the camera can
make the camera follow the wrong speaker. This happens when the speaker walks away from the center so the microphone
detects a large movement, but the object detector detects another person near the center of the camera view. The camera
gets two commands: one telling it to make a big movement and another telling it to make a small movement. Whenever the
camera receives one signal quickly after another, it stops its movement to follow the last command that it received.
This means that the big movement to the actual speaker is interrupted by the small movement of someone who is not the
speaker.
For this reason, we do not consider the thresholding strategy to be a good strategy in general. The two following strategies give a better user experience in all cases.

\paragraph{Waiting for small movements}\mbox{} \\
In order to avoid the problem of sending different signals at the same time, we considered disabling
object tracking when speaker tracking was running and vice versa. Since the speaker tracking should
still, be responsible for big movements in the system, we decided to wait till the speaker tracker
points to the same direction for a variable amount of iterations; one iteration means one time the
the microphone detects a sound louder than the defined threshold.
This means that object tracking is used when someone is not moving a lot, but it will never be used
simultaneously with speaker tracking. This helps to centralize the speaker on the screen.


\paragraph{Quick change object-audio strategy} \mbox{}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{tree}
    \caption{Possible states diagram}
    \label{fig:object_strategy}
    \caption{Possible states diagram}
\end{figure}
\\
The last approach we consider is always relying on microphone coordinations
when there is an active speaker, but furthermore adjust the camera based on
camera footage and bounding boxes. When there is an active speaker, we first
search for his location using the microphone information. If he is in the FoV
of the camera, we get the bounding speaker boxes from the current frame.
After translating the microphone direction to a camera direction and finding
its position on the screen as a pixel, we search which bounding box’s center is
the closest to that pixel. After finding it, we move the camera so that this
bounding box is in the middle of the camera screen.

If the speaker is not
currently visible on the screen, we again translate the microphone direction to
a camera direction and rotate the camera so that it faces that way. Finally,
in the case that there is no active speaker, we attempt to continue tracking
the last active speaker. In order to achieve that, we rely on the fact that the
bounding box of the last active speaker in the current frame should be the
closest to his new bounding box in the new frame. Relying on that fact, we get
all of the new bounding boxes from the new frame and choose to track the one
that has its center the closest to the previous bounding box. A diagram that
illustrates the possible states we could be in is the depicted in Figure
\ref{fig:object_strategy}. A visualization of this process can be seen in Figure \ref{fig:obj_detection_image}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{obj_detection_image}
    \fbox{\begin{tabular}{ll}
      \textcolor{red}{$\blacksquare$} & Currently tracked box \\
      $\blacksquare$ & Detected object boxes \\
      \textcolor{blue}{$\blacksquare$} & The microphone direction translated to camera direction 
    \end{tabular}}

    \caption{A frame recorded with quick change object-audio strategy}
    \label{fig:obj_detection_image}
\end{figure}

\section{Application overview}\label{sec:application-overview}
In this section, we will discuss our application's structure and our motivations behind it.
We go over each of the four modules in our project: the camera API, the microphone api, the tracking
module and the web interface. Each module is separated in the code in order to make it easier
for other developers to work on our code.

\subsection{Camera API}
The Camera API module contains all of the functionality related to
communication with the camera. This includes sending commands to move or zoom
the camera and get information about the current state of the camera.
In this section we discuss the communication method with the camera,
the design pattern used in the code and the way we get the footage
stream from the camera.

\paragraph{VISCA}\mbox{} \\
We chose to use VISCA commands to communicate with the camera instead of HTTP
requests. We did this first of all because Mr. Kahawati explained to us that it
was the standard at Avonic. The second reason was that VISCA was specifically
made for communication with PTZ cameras and therefore suits our purposes better
than HTTP. There are two types of VISCA requests: long and short. The
difference between these two types is the header. Long VISCA requests have a
header in the request telling the receiver how big the request is and what the
request counter is. This request counter is also contained in the response, so
the request and response can be matched. Short VISCA requests lack this header
and therefore do not provide the possibility to match responses to requests.

We started the project by using short VISCA requests. We had two reasons for
this: a) they are easier to implement and b) we had low traffic, so we could
assume that the first response we got was the response to the latest request.
As we started sending more requests to the camera, we found that many responses
got mixed up. For this reason, we switched to long VISCA requests for any
request whose response is used by the program. Such requests ask for
information about the camera such as its position, so every response has to be
matched to the correct request. There are other requests, such as moving the
camera, that do not require a response. For these requests, we still use short
VISCA requests.

\paragraph{Adapter pattern}\mbox{} \\
The implementation of the Camera API uses the adapter design pattern, which is a programming design pattern that allows different interfaces to work together\cite{adapter-pattern}.
The main advantage of this design pattern in our project is that it makes sure our system still works if the specifics of communication with the camera change; only the adapter itself has to be reimplemented. This makes it easier to adapt our system to other Avonic cameras.

\paragraph{Footage thread}\mbox{} \\
The final part of the Camera API module is the thread that reads video frames from the camera stream. 
This thread retrieves footage from Real Time Streaming Protocol (RTSP) stream of the camera using \verb|OpenCV|.
The frames are continually read by the thread so that the latest frame is always available in the buffer.
This frame can be requested by the web interface to show the stream to the user or by the tracking thread to localize the speaker.


\subsection{Microphone API}
The Microphone API module contains all of the functionality related to
communication with the microphone. The most important requests are the one that
gets the direction from the microphone towards the detected sound and the one
that gets the peak volume. Like the Camera API, the Microphone API uses an
adapter pattern to facilitate the use of other microphones in the future.

\subsection{Tracking}\label{subsection:tracking}
All of the functionality related to speaker tracking is contained in this
module. The most important part of it is the \verb|UpdateThread|. It is
responsible for continuously calling the method that directs the camera.
The module is structured in a way that every tracking type is maintained in a
so-called model. Every model is inherits from the \verb|TrackingModel| and has
the appropriate implementation of the point method. The user can pick their
preferred model in the web interface. Figure \ref{fig:model-use-case-diagram}
can be used to find out which model works best for a given scenario.
The chosen model is instantiated at the beginning of the \verb|run| method and
is subsequently used to track the speaker until being explicitly stopped by the user.
Some of the models rely on data stored in JSON files whose location can be
specified in the settings menu in our WebUI. We currently have the following
tracking models:
\begin{enumerate}
\item Preset Model
\item Audio Model
\item Object Tracking Model (wait object-audio model \& quick change object-audio model)
\end{enumerate}

\paragraph{Preset model}\mbox{} \\
The Preset Model makes use of a \verb|PresetCollection| which stores all
available preset locations. A preset location contains a name, the direction from the
microphone and the direction of the camera towards it. When fed a direction
from the microphone, this model finds the preset which is the closest to
the given microphone direction according to the cosine similarity. This
similarity metric compares the directions of vectors while ignoring their norm.
After finding the closest preset, the camera is pointed towards that preset
location.

\verb|PresetCollection| can be edited from the WebUI. This is how new preset
locations are added or old ones removed. Added preset locations are reloaded
from a file when the server starts, so there is no need to set preset locations
every time the server restarts.
This preset model is what is mostly used throughout the industry for streaming
web conferences and round-table meetings. The reason it is so widely used is
that it is simple to set up for people without technical background and the fact
that it is very precise in those scenarios. Setting up presets only requires
recording a couple of possible locations for the speaker in the room. Exactly
the simplicity of the installation and the wide usage are the reason why we
chose this model as a starting point for our tracking.

\paragraph{Audio model}\mbox{} \\
The Audio Model achieves the main goal of our application by continuously
tracking the speaker around the room. Unlike the preset model, it can point the
camera towards any arbitrary point in the room by performing the calculations
described in the subsection \ref{subsec:calibration} about calibration. The
audio model also keeps track of how much time has passed since someone has last
spoken. If no one has spoken in some time the camera will zoom out. This model’s
point method also takes into account how far the speaker is from the camera and
zooms in accordingly. Another improvement we implemented in this model is
changing the camera speed depending on how much it has to move. It helps reduce
jittering when following the speaker. The model also saves calculated vectors that were
added to the calibration and can load this data from a supplied file. If any of the
fields from that file are non-existent or corrupted, they are replaced with
default values.

\paragraph{Audio model without adaptive zooming}\mbox{} \\
This model has almost the same functionality as the Audio Model with the
difference that it does not have the ability to adapt the camera zoom depending
on the position of the speaker. It also eliminates the possibility for the
camera to zoom out when no one is speaking. This means that for the whole period
of tracking the zoom is not changed unless the user expressly does that using
any mode for controlling the zoom. We decided to add this model because in some
scenarios the adaptive zooming does not provide a very pleasant flow of the
tracking. Sometimes the camera zooms in or out unexpectedly or too much.
Reasons for that inconsistency are the possibility of inaccurate calibration or
inaccurate readings provided by the microphone.

\paragraph{Object-tracking model}\mbox{} \\
The Object Tracking Model is intended to refine our tracking in situations when
the speaker is not audible or the microphone’s direction towards the speaker is
not precise enough to accurately center the speaker. After having identified all
the people in the frame and having created the corresponding bounding boxes
around them it has a method to find the box that is most likely to be the one
around the speaker. Then another method calculates the relative direction in
which the camera has to move to center the box. It takes into account the
zooming and field of view of the camera in order to make the calculations. As
explained in section \ref{sec:visual-object-tracking} about Visual object
tracking, we chose to use YOLO for our object
detection and classification. Our YOLO implementation extends an abstract class
that has methods to calculate and draw bounding boxes. Any object detection
algorithm can extend this class in order for it to be used in place of the
currently implemented algorithm.

The object tracking models are often combined with the audio tracking model, so
two input streams (audio and video) have to be analyzed simultaneously. It would
seem that two threads are necessary in order to process both streams at the same
time. However, the time it takes to run the necessary calculations is less than
the required time to wait between two movement commands to the camera to avoid
interruptions. This means that running the calculations consecutively does not
impact the performance of the thread as a whole. Moreover, using just one thread
avoids code duplication and unnecessary complexity. Therefore, we decided to use
one thread that can run all models.

\paragraph{Model system}\mbox{} \\
For the tracking structure we settled on using an abstract class
\verb|TrackingModel| which all our different tracking models extend and
implement. The reason for this is to not duplicate functionality when possible
and to allow for the easy development of different tracking models. This
structure gives the possibility to future developers to just extend our
functionality without having to refactor it.


\subsection{Web UI}\label{subsection:solution_webui}
In order to support the web user interface (UI) for the system, the web application
needed to be created. The back-end of this app is written in Python using the
Flask framework as it is a well-known lightweight framework, that allows for web
development. It offers some nice features -- creating and managing socket
connections with a client and easy modification to support testing of the app.
Flask is also used by Avonic themselves and was recommended by them.

The front-end is written in HTML, CSS, and JavaScript and includes controls for
the camera, controls for the tracking thread, and an editing interface for
presets and calibration models. The page maintains a WebSocket connection with a
server for footage stream and real-time updates of the camera and microphone
information, which is used for visualization.

\paragraph{Web UI requirement}\mbox{} \\
The web UI was moved from a “should have” to a “must have” section after
consultation with our TA, as it provides a clearer, more intuitive, and more
pleasant way for the user to interact with a system. It also opened up space for
more quality-of-life features, that we have not intended in the original
requirements, but found useful for future users and debugging purposes -- footage
stream, real-time updates of microphone information and camera position, and
visualization.

\paragraph{Calibration interface}\mbox{} \\
The part of our web interface that guides users through the process of
calibration is shown in figure \ref{fig:calibration-ui}. As is shown, there are
at most three calibration points that can be set. After at least one point is
set, the camera direction towards the microphone can be submitted. The reason we
decided to have a maximum of three points is to give a reasonable guideline to
the user. From experiments that we have conducted, adding more than three points to calibration does not make it more
accurate. We found that one calibration point gives good accuracy, which is why we allow
the user to end the process after recording one point. The main advantage of
recording more than one point is to reduce the impact of inaccurate microphone
directions.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{calibration_ui}
    \caption{Part of the web interface responsible for calibration}
    \label{fig:calibration-ui}
\end{figure}

\paragraph{Integration object}\mbox{} \\
In the first weeks of working on the project, we experienced multiple issues
related to sharing access to Camera and Microphone APIs across the endpoints, as
we could not find a dependency injection solution that would work with modern
versions of Flask. We decided to unite all objects that needed to be shared
across the endpoints in a single object and make it accessible for the server
application — the \verb|GeneralController| object, also referred to as
``integration’’. It contains variables for thread flow control, camera and
microphone APIs and all of the threads our application uses -- update thread,
responsible for tracking, microphone and camera info-threads, responsible for
sending information updates to the user, footage thread for retrieving frames
from the camera.

As it is not possible by default to create global variables in Flask, we have
decided to use vanilla Python features. All of the definitions of the endpoints
were moved to the \verb|__init__.py| file, so the integration object can be
passed to every endpoint’s body function. To avoid the bloating of the file, we
agreed on the rule that every endpoint definition should contain exactly one
line with the call of the respective function with the actual body of the
endpoint, which can be found in other files in the WebUI package. After
consultation with Avonic, they told us that they had similar issues with
dependency injection in Flask, and they expect from us a functional, not
necessarily clean solution for this issue.

\paragraph{Production server}\mbox{} \\
By default, Flask applications are run in a single-process development mode,
which does not use WebSockets. We have decided to pick uWSGI as our production
server, as it is well documented, fast, compiled, and gets rid of the downsides
of the development server. A script was created to easily run the server in
different modes: with tests, development mode and production mode.
The script also allows to run the project without footage stream, by supplying 
\verb|NO_FOOTAGE=true| as the environment variable.
