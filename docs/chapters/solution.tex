This chapter aims to present our motivations when developing a solution, a detailed description of the solution that our team picked for the project and other possibilities which we considered, but did not choose, and the reasoning behind why not.
\section{Challenges that must be overcome}
\subsection{Technical limitations}
As outlined in the requirements, the system should act as a middleware device that can be put on the same network as the microphone and the camera, which adds a limitation on the amount of available resources.
We could rely on cloud computing power, but that would lead to a big delay in camera movement and additional risks to security and maintainability.
Additionally, our project relies on 2 special hardware devices -- a camera, and a microphone, therefore the final product should account for all of those limitations.
Unfortunately, the microphone does not provide location information, as it is quite a difficult technical challenge\cite{distance-urbana-champaign}.
So the list of important limitations, that significantly impact the project can be formulated with three limitations:
\begin{itemize}
    \item The system should run on a single-board computer (SBC)
    \item The system should be able to run on the same network as a camera and a microphone.
    \item Microphone does not provide distance information
\end{itemize}
\subsection{User limitations}
In addition to technical criteria, the main criterion of the solution we were going to implement is based on a certain non-functional requirement which client has emphasized multiple times during our meetings:
\begin{itemize}
    \item The system should be easy to install and calibration can be done by teachers or presenters themselves, it should not require any technical knowledge or external devices.
\end{itemize}
As this criterion can be interpreted ambiguously, as the difficulty of calibration can vary per person, we needed to reiterate our solutions multiple times and verify them with Avonic.
Meeting this criterion was difficult, as we spent the first week of our project solely working on different solutions, trying to prove their correctness, while not disrupting their calibration and installation simplicity.
\section{Implemented solution}
In this section we give the motivation and reasoning behind our implemented solution.
\subsection{Assumptions}
As described earlier, there are certain challenges that we needed to overcome in order to devise a working solution.
Having brainstormed different implementation strategies,
we identified that the most difficult limitation is that the microphone array cannot retrieve distance information.
This, combined with the fact that the microphone array is in a separate position from
the camera, meant that generally finding out where the speaker is actually located becomes impossible without
hacking/reverse-engineering it, since we do not have access to each individual microphone within the array.
That can be easily shown when one imagines a speaker, who starts off standing directly beneath the microphone array,
sitting down. Our program will then not be able to discern whether to tilt the camera up or down based solely on
the information provided by the Ceiling.

This means that we need to make assumptions about the environment of the room:


\begin{enumerate}
    \item Distance between the speaker and the plane of the ceiling, to which the microphone is mounted, is constant.
    \item If a camera, presenter's plane, or a microphone were moved, the system must be recalibrated.
    \item The system is well-calibrated and no components of it are moved during the tracking.
\end{enumerate}
Through the process of the research and reiteration of our proposals, with the amount of time we are given, taking in account all the limitations, we have identified these assumptions as minimal.
While assumptions 2 and 3 are in place to guarantee the intended way of using the system, assumption 3 is formulated to determine the distance from the microphone to the speaker.
From all of our attempts to come up with the solution, we have identified that the distance of the vector from the microphone to the speaker is crucial.
But as our system is closed under only 2 devices, and the microphone's API\cite{microphone-manual} does not have a way to retrieve distance, the assumptions are needed to somehow describe the distance function in the environment.
We have chosen the scenario where we look for an intersection of the ray from the microphone with the ``speaker plane'' as the plane is the most probable scenario for the presenter's environment.
It is the most probable case of how the available place for the presenter on stage looks; small inconsistencies in not precisely a flat floor can be neglected due to their small magnitude \textbf{(MAYBE PUT REFERENCE HERE)}.
The height difference between the speakers can be neglected, due to the expected big distance from the camera to the presenters, and we expect it to be insignificant, especially in scenarios where people are sitting \textbf{(MAYBE PUT REFERENCE HERE)}.
In the future, it is possible to extend the model to support different shapes and possibly a full 3D scan of the room, but we view it to be infeasible for us in 10 weeks.
\subsection{Calibration}
During the calibration stage, we calculate the distance from the camera to the microphone and therefore obtain the camera's relative coordinates to the microphone.
We assume the longest one of the horizontal coordinates to be the $z$ one.
To achieve this, the microphone's height is first set.
Then, a person is sent into the room, they direct the camera at themselves and speak.
The direction obtained from the camera and microphone combined with the microphone's height above the so-called speaker plane is enough to calculate all the speaker plane's properties and 3D position.

A simplified, 2D image is shown in figure \ref{fig:cal2d}.
\begin{figure}[h]
    \caption{Demonstration of the calculation of calibration in 2D}
    \centering
    \includegraphics[width=0.8\textwidth]{calibration_2d}
    \label{fig:cal2d}
\end{figure}
There, we say $M$ is the microphone, $C$ is the camera, and $A$ is the person calibrating the system.
After directing the camera at the speaker, we find out angles $\alpha$, $\beta$ and distance $a$ from the right-angled triangle between the speaker plane, the one perpendicular to it from the microphone, and the speaker.
Angle $\delta$ is easily calculated from the fact that the primary axes of the camera and microphone are orthogonal, and because we know $\gamma$ from having directed the camera at the microphone in the second stage of the calibration.
Now that we know 2 angles and one side length of the triangle $CAM$, we can calculate the distance between the camera and microphone -- $d$.

After calibration, we can proceed backwards to calculate the angle to point the camera at from $d$.
Figure \ref{fig:cal3d} contains a visualization of the same approach, but with vectors instead of angles.
We can calculate distance $|MA|$ by intersecting the microphone's direction vector with the speaker plane.
Afterwards, we can use analogous calculations to get the length $|CM|$, since we have all the directions we need.
This means that we can find the angles between them and calculate the triangle.
\begin{figure}[h]
    \caption{Calculation of calibration in 3D}
    \centering
    \includegraphics[width=0.8\textwidth]{calibration_3d}
    \label{fig:cal3d}
\end{figure}
\section{Alternative approaches}
\subsection{Single coordinate system}
One of the most obvious solutions that we came up with, is providing the system with coordinates of both microphone and the camera, in coordination with the microphone providing distance to the speaker.
As you can see, this solution served as inspiration for the final solution, but it has two important problems:
\begin{itemize}
    \item Distance information from the microphone is not obtainable.
    \item Manually inputting measures that would describe coordinate system and points of the camera and the microphone, was considered too complex by Avonic.
\end{itemize}
\subsection{Interpolation model}
One of the solutions we came up with, was inspired by a concept of interpolation, first introduced to us in the Computer Graphics course.
The proposed solution followed this algorithm for calibration and tracking:
\begin{enumerate}
    \item Record three positions by retrieving the beams' directions and the camera's position for each of the recorded placements.
    \item Depending on the angle of the beam, using angles that describe its direction, translate those angles to angles of the camera and point it towards the derived direction.
\end{enumerate}
Although this solution might sound fascinating and simple, it does not work in reality.
This algorithm would only be possible if the camera was located at the same point as the microphone, or in completely opposite position from the microphone, in relation to the calibration points.
This can be proven with a geometric proof, or using geometric modeling that we did with GeoGebra\cite{geogebra}.
Unfortunately, this is not always the case, as the system should work with any arbitrary position of the camera, so we had to leave this solution behind.
\subsection{Room scan}
This was one of the more ambitious solutions we considered at the beginning of the project.
It was somewhat inspired by the solution with preset locations.
The idea is that we input all the possible planes on which the speaker could be located.
For example, if there is an elevated scene somewhere at the front of the room we input its size and elevation from the floor.
Also, there is the possibility to add locations for chairs, stairs, etc.
This way we have some kind of a room scan with all the objects’ arrangements inside.
This scan gives us a more precise idea of where the speaker could be actually located.
However, this solution has two quite major drawbacks.
First of all, the direction from the microphone could still result in multiple possible locations of the speaker when considering its intersection with all the given planes.
The other disadvantage of this approach is that it requires a lot of work to set up the system and as we have stated earlier this does not align with our requirements.


\section{Visual object tracking}
This section discusses the addition of visual object tracking to our audio-based speaker tracking
system. The combination of both visual and audio information can make the movement of the camera
smoother and more accurate. Audio discriminates between different people to determine who is
speaking; video helps to exactly center the camera on this person's face.
\subsection{Object tracking methods}
For the visual object tracking part of our system, we considered two methods: YOLO and RT-DETR. Both
are neural networks trained to localize and classify objects in a video. The reason we looked at
neural networks instead of traditional image processing techniques is because neural networks
achieve greater accuracy when large training sets are
available\cite{image-processing-vs-deep-learning}. Such training sets (e.g. VOC 2007\cite{pascal-voc-2007} and MS COCO\cite{microsoft-coco}) are
available for object detection and many models have been trained on these sets. We elaborate on two of such models below.


\paragraph{YOLO}\mbox{} \\
The YOLO (You Only Look Once) network is one of the most common ways to track objects in real-time. YOLO sees the
problem of finding bounding boxes and classifying objects in an image as a regression problem\cite{you-only-look-once}.
The input image is divided into a grid. The network predicts three things for each box in the grid:
First, a constant number of bounding boxes that have their center in the grid. Second, a number
representing the class the object in
the box belongs to and a confidence score for that class prediction. As such, the output of the network is a vector of
constant size. Since the network often outputs many overlapping boxes, non-maximum suppression is
used. This is a technique that takes a group of boxes with the center in the same square and only
keeps the best performing one. The performance of objects during non-maximum suppression is based on
the intersection-over-union score. This is the score calculated by intersecting one boxes with the
other boxes and dividing this by the union of the same boxes; the higher the score, the more
accurate the prediction.

Considering the YOLO network was made for real-time performance, it fits well with our system. It can process {test this
with Jetson} frames per second. We do not want to track objects that are not people and the classification part of the
network helps to do this.


\paragraph{RT-DETR}\mbox{} \\
The Real-Time Detection Transformer is the first real-time transformer-based detector. In contrast to the convolutional
neural network (CNN) detectors, transformer-based detectors do not need a post-processing step with non-maximum
suppression. This way, RT-DETR manages to achieve a higher speed than YOLO while not losing any accuracy\cite{rt-detr}.

Just like YOLO, RT-DETR can be used to track objects in real-time to assist our overall speaker tracking. However,
despite its claim to improved performance, we noticed that in practice the YOLO algorithm was multiple times faster
than the RT-DETR. This is why we ended up using YOLO as our object detector.


\subsection{Integration of object tracking into our speaker tracker}
The problem of combining two systems of tracking a speaker, both object tracking and speaker tracking, is discussed in
this section. A decision has to be made by the system whether to listen to the speaker tracker or the object tracker.
There are three strategies we implemented in our system to deal with this problem. We review each of them and explain in what kind of situation the strategies work best. Avonic has asked us to give audio-based speaker tracking priority over object detection using video frames. Because of this, all strategies we tried to use audio to determine the location of the speaker for the most important (i.e. biggest) movements.
Since the last two strategies both improve speaker tracking in different ways, the web UI gives users the option to choose between these methods.

\paragraph{Thresholding of camera movements}\mbox{} \\
The first solution we came up with was to threshold incoming commands. This means that we ignore the speaker tracking if
it wants the camera to move a number of degrees lower than the threshold and we ignore the object tracking if it tells
the camera to move a number of degrees higher than the threshold. The envisioned result of this approach would be that
the speaker tracking moves the camera towards the correct speaker and the object tracker then follows the small
movements this speaker makes. This uses both the microphone's ability to detect which person is speaking and the object
detector's ability to precisely localize a person on a frame.

When we tested this strategy, we found out that the simultaneous signals that are still being sent to the camera can
make the camera follow the wrong speaker. This happens when the speaker walks away from the center so the microphone
detects a large movement, but the object detector detects another person near the center of the camera view. The camera
gets two commands: one telling it to make a big movement and another telling it to make a small movement. Whenever the
camera receives one signal quickly after another, it stops its movement to follow the last command that it received.
This means that the big movement to the actual speaker is interrupted by the small movement of someone who is not the
speaker.
For this reason, we do not consider the thresholding strategy to be a good strategy in general. The two following strategies give a better user experience in all cases.

\paragraph{Waiting for small movements}\mbox{} \\
In order to avoid the problem of sending different signals at the same time, we considered disabling
object tracking when speaker tracking was running and vice versa. Since the speaker tracking should
still, be responsible for big movements in the system, we decided to wait till the speaker tracker
points to the same direction for a variable amount of iterations; one iteration means one time the
the microphone detects a sound louder than the defined threshold.
This means that object tracking is used when someone is not moving a lot, but it will never be used
simultaneously with speaker tracking. This helps to centralize the speaker on the screen.


\paragraph{Switch strategy} \mbox{} 
\begin{figure}[h]
    \caption{Possible states diagram}
    \centering
    \includegraphics[width=0.5\textwidth]{tree}
    \label{fig:object_strategy}
\end{figure}
\\
The last approach we consider is always relying on microphone coordinations when there is an active speaker, but furthermore adjusts the camera based on camera footage and bounding boxes. When there is an active speaker, we first search for his location using the microphone information. If he is in the FoV of the camera, we get the bounding speaker boxes from the current frame and after translating the microphone direction to a camera direction and finding its position on the screen as a pixel, we search which bounding box’s center is the closest to that pixel. After finding it, we move the camera so that this bounding box is in the middle of the camera screen. If the speaker is not currently visible on the screen, we again translate the microphone direction to a camera direction and rotate the camera so that it faces that way. And finally in the case that there is no active speaker, we attempt to continue tracking the last active speaker. In order to achieve that, we rely on the fact that the bounding box of the last active speaker in the current frame should be the closest to his new bounding box in the new frame. Relying on that fact, we get all of the new bounding boxes from the new frame and choose to track the one that has its center the closest to the previous bounding box.
A diagram that illustrates the possible states we could be in is the depicted in Figure \ref{fig:object_strategy}.


\section{Application overview}
In this section, we will discuss our application's structure and our motivations behind it.
\subsection{Project modules}
After splitting our project into core domains, we have identified 4 of them:
\begin{itemize}
    \item Camera API - responsible for communication with the camera.
    \item Microphone API - responsible for communication with the microphone.
    \item Tracking - contains all the tracking models we came up with, UpdateThread interface which can run any model supplied, and utilities for future development of different tracking strategies.
    \item WebUI - responsible for providing a user-friendly interface of the system.
\end{itemize}
We took a decision to create separate Python modules for each of the identified domains, to make our code base more systematic.
It also allows us to potentially distribute some modules separately, allowing other people to develop their software using our APIs.
\subsection{Camera API}
The Camera API module contains all of the functionality related to communication with the camera. This includes sending commands to move or zoom the camera and get information about the current state of the camera.
\paragraph{VISCA}\mbox{} \\
We chose to use VISCA commands to communicate with the camera instead of HTTP requests. We did this first of all because Mr. Kahawati explained to us that it was standard at Avonic. The second reason was that VISCA is faster than HTTP [REFERENCE?!].
There are two types of VISCA requests: long and short. The difference between these two types is the header. Long VISCA requests have a header in the request telling the receiver how big the request is and what the request counter is. This request counter is also contained in the response, so the request and response can be matched. Short VISCA requests lack this header and therefore do not provide the possibility to match responses to requests.
We started the project by using short VISCA requests. We had two reasons for this: a) they are easier to implement and b) we had low traffic, so we could assume that the first response we got was the response to the latest request. As we started sending more requests to the camera, we found that many responses got mixed up. For this reason, we switched to long VISCA requests for any request whose response is used by the program. Such requests ask for information about the camera such as its position, so every response has to be matched to the correct request. There are other requests, such as moving the camera, that do not require a response. For these requests, we still use short VISCA requests.
\paragraph{Adapter pattern}\mbox{} \\
The implementation of the Camera API uses the adapter design pattern, which is a programming design pattern that allows different interfaces to work together\cite{adapter-pattern}.
The main advantage of this design pattern in our project is that it makes sure our system still works if the specifics of communication with the camera change; only the adapter itself has to be reimplemented. This makes it easier to adapt our system to other Avonic cameras.
\paragraph{Footage thread}\mbox{} \\
The final part of the Camera API module is the thread that reads video frames from the camera stream. This thread is connected to the RTSP stream of Real Time Streaming Protocol (RTSP) stream of the camera that emits one hundred frames per second. These frames are continually read by the thread so that the latest frame is always available. This frame can be requested by the web interface to show the stream to the user or by the tracking thread to localize the speaker.

\subsection{Microphone API}
The Microphone API module contains all of the functionality related to communication with the microphone. The most important requests are the one that gets the direction from the microphone towards the detected sound and the one that gets the peak volume. Like the Camera API, the Microphone API uses an adapter pattern to facilitate the use of other microphones in the future.

\subsection{Tracking}
All of the functionality related to speaker tracking is separated in this module. The most important part of it is the updater. It starts a thread that is responsible for keeping track of which tracking model is currently used and regularly calls the corresponding point method which directs the camera. Also, if there are files with calibration or presets existing, the updater will instantiate the tracking models with them. They are stored in JSON files whose location can be specified in the settings menu in our WebUI.
Since we have different ways of performing the tracking, we decided to create an abstract tracking model which has the point method. This method is responsible for calculating the direction in which the camera should point at the given moment and sends the corresponding request to move it. Each of our tracking models has its specific implementation for pointing which gives the ability to add more models in the future. We currently have the following tracking models:
\begin{enumerate}
\item Preset Model
\item Audio Model
\item Object Tracking Model
\end{enumerate}

\paragraph{Preset model}\mbox{} \\
The Preset Model makes use of a Preset Collection which stores all available preset locations. They are specified by a name, a direction from the microphone, and the direction of the camera towards them. When fed a direction from the microphone, the point method in this model finds the preset with the closest microphone direction to the one we have. This is calculated using cosine similarity. After finding the closest preset we point the camera towards that preset location. In the preset.py file, we have the preset addition/deletion/editing which can be done from the WebUI as well as the ability to load them from the already specified file. This preset model is what is mostly used throughout the industry for streaming web conferences and round-table meetings. The reason it is so widely used is that it is simple to set up for people without technical background and the fact that it is very precise in those scenarios. Setting up only requires recording a couple of possible locations for the speaker in the room.
\paragraph{Audio model}\mbox{} \\
The Audio Model achieves the main goal of our application by continuously tracking the speaker around the room. Unlike the preset model, it can point the camera towards any arbitrary point in the room by doing some calculations. It first gets the direction from the microphone towards the speaker. Then the point method makes use of another method that converts this direction to the corresponding camera direction given the calibration we currently have. The audio model also keeps track of how much time has passed since someone has last spoken. If no one has spoken in some time the camera will zoom out. This model’s point also takes into account how far the speaker is from the camera and zooms in accordingly if this option is enabled. Another improvement we implemented here is changing the camera speed depending on how much it has to move.
This module also contains the calibration.py file. It is responsible for setting up all the information needed for calibrating the system such as setting the microphone height from the speaker plane and recording calibration points. After having this information another method calculates the vector from the microphone towards the camera which enables us to convert the microphone direction to the camera direction as mentioned earlier. This file takes care of loading an existing calibration if there is such provided. If any of the fields from that file are non-existent or corrupted they are replaced with default values.
\paragraph{Object-tracking model}\mbox{} \\
The Object Tracking Model is intended to refine our tracking in situations when the speaker is not audible or the microphone’s direction towards the speaker is not precise enough to accurately center the speaker. After having identified all the people in the frame and having created the corresponding bounding boxes around them it has a method to find the box that is most likely to be the one around the speaker Then another method calculates the relative direction in which the camera has to move to center the box. It takes into account the zooming and field of view of the camera in order to make the calculations.
As explained in section [SECTION NUMBER OF OBJECT TRACKING] we chose to use YOLO for our object detection and classification. Our YOLO implementation extends an abstract class that has methods to calculate and draw bounding boxes. Any object detection algorithm can extend this class in order for it to be used in place of the currently implemented algorithm.

The object tracking models are often combined with the audio tracking model, so two input streams (audio and video) have to be analyzed simultaneously. It would seem that two threads are necessary in order to process both streams at the same time. However, the time it takes to run the necessary calculations is less than the required time to wait between two movement commands to the camera to avoid interruptions. This means that running the calculations consecutively does not impact the performance of the thread as a whole. Moreover, using just one thread avoids code duplication and unnecessary complexity. Therefore, we decided to use one thread that can run all models.


\subsection{WebUI}
In order to support the web user interface for the system, the web application needed to be created.
The back-end of this app is written in Python using the Flask framework.
We have decided to use Flask as it is a well-known lightweight framework, that allows for web development.
It offers some nice features - creating and managing socket connections with a client for automatically updating values of the system and being easily modifiable to support testing of the app.
Flask is also used by Avonic themselves and was recommended by the client.
The front-end is written in HTML, CSS, and JavaScript and includes controls for the camera, controls for the tracking thread, and an editing interface for presets and calibration models.
The page maintains a WebSocket connection with a server for footage stream and real-time updates of the camera and microphone information, which is used for visualization.

The WebUI was moved from a “should have” to a “must have” section after consultation with our TA, as it provides a clearer, more intuitive, and more pleasant way for the user to interact with a system. It also opened up space for more quality-of-life features, that we have not intended in the original requirements, but found useful for future users and debugging purposes - footage stream, real-time updates of microphone information and camera position, and visualization.

In the first weeks of working on the project, we experienced multiple issues related to sharing access to Camera and Microphone APIs across the endpoints, as we could not find a dependency injection solution that would work with modern versions of Flask. Because, we decided to not use HTTP for main camera communication, as repetitive socket creation and connection is an expensive operation, sacrifice should have been made to the Flask server, as the framework does not support dependency injection by default.

We decided to unite all objects that needed to be shared across the endpoints in a single object and make it accessible for the server application — the \\ \verb|GeneralController| class, also referred to as ``integration’’. It contains variables for thread flow control, camera and microphone APIs all of the threads our application uses - update thread, responsible for tracking, microphone and camera info-threads, responsible for sending information updates to the user, footage thread for retrieving frames from the camera.

As it is not possible by default to create global variables in Flask, we have decided to use vanilla Python features. All of the definitions of the endpoints were moved to the \verb|__init__.py| file, so integration can be passed to every endpoint’s body function. To avoid the bloating of the file, we agreed on the rule that every endpoint definition should contain exactly one line with the call of the respective function with the actual body of the endpoint, which can be found in other files in the WebUI package. After consultation with Avonic, they told us that they had similar issues with dependency injection in Flask, and they expect from us a functional, not necessarily clean solution for this issue.

By default, Flask applications are run in a development mode, which does not use WebSockets, and single processed. We have decided to pick uWSGI, as it is well documented, fast, compiled, better manages resources, and gets rid of the downsides of the development server. To correctly install packages and run the server in the most used variations, the \verb|run.sh| script was created that supports running tests, the server in development, and production modes.
