We have developed and described the tracking models that might be useful in different scenarios: Preset model - works best when speakers are only present in a few locations, and the Audio tracking model acts best in scenarios where speakers do not change the height of their locations, and the Audio tracking model combined with object tracking works best in cases where there are little to no restrictions on how speakers can move. We have discussed the challenges behind their implementations, mostly related to restrictions of the system’s setup procedure, the motivation behind them, and the description of their behavior. In addition to that, a model system was developed, that allows Avonic or third-party developers to develop and easily integrate their models into our system.


To facilitate all of the above-mentioned tracking models were developed Camera and Microphone APIs to communicate with hardware that is used for tracking. In addition to that, to accommodate users with different levels of knowledge the Flask application with Web interface was developed, that allows to access all of the Camera and Microphone APIs features, calibrate and control the flow of tracking models, and provides the user with additional information for better understanding of the current state of the system - visualization of camera’s and microphone’s directions and footage streamed from the camera. APIs and the WebUI can be distributed as separate packages that would allow engineers at Avonic easily integrate our solution with their possible modifications.


We have discussed and analyzed our workflow practices during the project. We have implemented a good testing methodology with multiple levels of testing that include mutation testing. Additionally, we have closely collaborated with Avonic which allowed us to have a more detailed overview of the envisioned functionality and helped us with hardware-related problems we were having along the way. We have discussed the challenges that we met during the project and how we resolved them. Lastly, we have reflected on our design choices, and how some of them can be improved in our next projects, or in the next iteration of this software. 


This report is delivered together with a code base and extensive documentation of its files and instructions on how to run it. In future multiple things can be improved, which can vary from an engineering-only to research-related - create actual dependency injection and port YOLO to TensorFlow to make it faster, or develop new strategies for tracking, as well as new algorithms for microphone arrays that would allow for obtaining the distance to the speaker from the microphone.
